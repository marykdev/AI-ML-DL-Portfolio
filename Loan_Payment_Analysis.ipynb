{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2232033,"sourceType":"datasetVersion","datasetId":1340957}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/maryamkamelan/loan-payment-analysis?scriptVersionId=204486762\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<div style=\"background: linear-gradient(90deg, #4b79a1, #283e51); padding: 20px; text-align: center; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px;\">\n    <h1 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 36px; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n        Loan Payment Analysis\n    </h1>\n    <br>\n    <p style=\"color: #d1ecf1; font-family: 'Arial', sans-serif; font-size: 18px; margin: 5px 0 0; text-shadow: 1px 1px 3px rgba(0,0,0,0.2); text-align: center;\">\n        Comparing Classical Machine Learning Algorithms with Deep Learning.\n    </p>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background: linear-gradient(90deg, #232526, #414345); padding: 20px; border-radius: 10px; box-shadow: 0 4px 12px rgba(0,0,0,0.15); margin-bottom: 20px;\">\n    <h2 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 28px; margin-bottom: 50px; text-align: center; text-shadow: 2px 2px 4px rgba(0,0,0,0.2);\">\n        📑 Table of Contents\n    </h2>\n    <ul style=\"list-style-type: none; padding: 0; font-family: 'Arial', sans-serif; font-size: 20px; color: white;\">\n        <li style=\"margin: 20px 0;\">\n            <a href=\"#section0\" style=\"text-decoration: none; color: #fff; background: #37474f; padding: 20px 15px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); display: inline-block; width: 100%\">\n                0. Introduction 📘\n            </a>\n        </li>\n        <li style=\"margin: 20px 0;\">\n            <a href=\"#section1\" style=\"text-decoration: none; color: #fff; background: #263238; padding: 20px 15px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); display: inline-block; width: 100%\">\n                1. EDA 📊\n            </a>\n        </li>\n        <li style=\"margin: 20px 0;\">\n            <a href=\"#section2\" style=\"text-decoration: none; color: #fff; background: #37474f; padding: 20px 15px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); display: inline-block; width: 100%\">\n                2. Data Preprocessing 🧹\n            </a>\n        </li>\n        <li style=\"margin: 20px 0;\">\n            <a href=\"#section3\" style=\"text-decoration: none; color: #fff; background: #263238; padding: 20px 15px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); display: inline-block; width: 100%\">\n                3. Classical Machine Learning Models 🤖\n            </a>\n        </li>\n        <li style=\"margin: 20px 0;\">\n            <a href=\"#section4\" style=\"text-decoration: none; color: #fff; background: #37474f; padding: 20px 15px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); display: inline-block; width: 100%\">\n                4. Deep Learning Models 🧠\n            </a>\n        </li>\n        <li style=\"margin: 20px 0;\">\n            <a href=\"#section5\" style=\"text-decoration: none; color: #fff; background: #263238; padding: 20px 15px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); display: inline-block; width: 100%\">\n                5. Conclusion 🔚\n            </a>\n        </li>\n    </ul>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div id=\"section0\" style=\"background-color:#1e1e1e; padding: 20px; border-radius: 10px;\">\n    <h2 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 32px; text-shadow: 2px 2px 4px rgba(0,0,0,0.2); text-align: center; margin-bottom: 12px\">\n        Introduction 📘\n    </h2>\n  <h1 style=\"color:#ff6f61; text-align:center;\">Loan Data Analysis with Machine Learning and Deep Learning Models</h1>\n  <p style=\"color:#dcdcdc; font-size: 18px;\">\n    In this project, we aim to analyze a <strong>loan dataset</strong> using various <span style=\"color:#8be9fd;\">machine learning</span> and <span style=\"color:#50fa7b;\">deep learning</span> algorithms. The objective is to compare the performance of these models to determine which one provides the best results in predicting loan outcomes. 📊🔍\n  </p>\n  <p style=\"color:#dcdcdc; font-size: 18px;\">\n    We will start by exploring the dataset, performing necessary data preprocessing, and then implementing several models including:\n  </p>\n  <ul style=\"color:#dcdcdc; font-size: 18px;\">\n      <li><span style=\"color:#8be9fd;\">🔹 Decision Tree</span></li>\n      <li><span style=\"color:#8be9fd;\">🔹 SVM</span></li>\n      <li><span style=\"color:#8be9fd;\">🔹 KNN</span></li>\n      <li><span style=\"color:#8be9fd;\">🔹 Random Forest</span></li>\n      <li><span style=\"color:#8be9fd;\">🔹 CatBoost</span></li>\n      <li><span style=\"color:#50fa7b;\">🔹 Dense Neural Network</span></li>\n      <li><span style=\"color:#50fa7b;\">🔹 CNN</span></li>\n      <li><span style=\"color:#50fa7b;\">🔹 ResNet</span></li>\n  </ul>\n  <p style=\"color:#dcdcdc; font-size: 18px;\">\n    🔸 Our analysis will include performance metrics such as accuracy, precision, recall, and F1-score to provide a comprehensive comparison of these models. 🌟📈\n  </p>\n  <p style=\"color:#dcdcdc; font-size: 18px;\">\n    ✳️ Let's dive into the data and discover insights that could help in making informed decisions regarding loan approvals. 💼💡\n  </p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"pip install tensorflow","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install keras-tuner","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install catboost","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n#import packages\n\n# Data loading and data management\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Data exploration\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import uniform, randint\n\n# feature engineering\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import LabelEncoder\nfrom imblearn.combine import SMOTETomek\n\n# ML\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import KFold, cross_val_score, RandomizedSearchCV, GridSearchCV\nfrom sklearn.model_selection import learning_curve\n\n# Deep learning\nimport tensorflow as tf\nimport keras_tuner as kt\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Conv2D, MaxPool2D, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nfrom tensorflow.keras import layers, callbacks ","metadata":{"execution":{"iopub.status.busy":"2024-10-30T16:00:33.178921Z","iopub.execute_input":"2024-10-30T16:00:33.180312Z","iopub.status.idle":"2024-10-30T16:00:33.194011Z","shell.execute_reply.started":"2024-10-30T16:00:33.180252Z","shell.execute_reply":"2024-10-30T16:00:33.192425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/loan-data/loan_data.csv\")\ndata.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-30T16:00:40.483864Z","iopub.execute_input":"2024-10-30T16:00:40.485089Z","iopub.status.idle":"2024-10-30T16:00:40.612532Z","shell.execute_reply.started":"2024-10-30T16:00:40.485024Z","shell.execute_reply":"2024-10-30T16:00:40.611333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #e7f3fe; border-left: 4px solid #2196F3; padding: 10px; margin: 20px 0;\">\n        <p style=\"color: #555; line-height: 1.6; text-align: justify;\">\nHere are what the columns represent:\n\ncredit.policy: 1= if the customer meets the credit underwriting criteria of LendingClub.com, and 0= otherwise.\n\n\npurpose: The purpose of the loan (takes values \"credit_card\", \"debt_consolidation\", \"educational\", \"major_purchase\", \"small_business\", and \"all_other\").\n\n\n int.rate: The interest rate of the loan, as a proportion (a rate of 11% would be stored as 0.11). Borrowers judged by LendingClub.com to be more risky are assigned higher interest rates.\n\n\ninstallment: The monthly installments owed by the borrower if the loan is funded.\n\n\nlog.annual.inc: The natural log of the self-reported annual income of the borrower.\n\n\ndti: The debt-to-income ratio of the borrower (amount of debt divided by annual income).\n\n\nfico: The FICO credit score of the borrower.\n\n\ndays.with.cr.line: The number of days the borrower has had a credit line.\n\n\nrevol.bal: The borrower's revolving balance (amount unpaid at the end of the credit card billing cycle).\n\n\nrevol.util: The borrower's revolving line utilization rate (the amount of the credit line used relative to total credit available).\n\n\ninq.last.6mths: The borrower's number of inquiries by creditors in the last 6 months.\n\n\ndelinq.2yrs: The number of times the borrower had been 30+ days past due on a payment in the past 2 years.\n\n\npub.rec: The borrower's number of derogatory public records (bankruptcy filings, tax liens, or judgments).\n            </p>","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #e7f3fe; border-left: 4px solid #2196F3; padding: 10px; margin: 20px 0;\">\n        <p style=\"color: #000; line-height: 1.6; text-align: justify;\">\n            The feature revol.bal (The borrower's revolving line utilization rate) has the highest standard deviation and so, it expected that this variable will contain outliers.\nOther features such as days.with.cr.line, installment, fico, and revol.util also show high standard deviations, as such, outliers in this data have to be detect and handled.\n         </p>","metadata":{}},{"cell_type":"markdown","source":"<div id=\"section1\" style=\"background:linear-gradient(90deg, #004d40, #00796b); padding: 20px; text-align: center; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px;\">\n    <h1 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 36px; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n        EDA\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Histograms for numeric features\ndf_numeric = data.drop(['purpose','credit.policy','not.fully.paid',], axis=1)\n\nnum_features = len(df_numeric.columns)\nnum_rows = (num_features + 2) // 3  # Calculate number of rows needed\n\nplt.figure(figsize=(12, 10))\n\nfor index, feature in enumerate(df_numeric):\n    plt.subplot(num_rows, 3, index+1)  # Adjust subplot creation dynamically\n    sns.histplot(data[feature], bins=30, kde=True)\n    plt.title(f'Distribution of {feature}')\n\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #e7f3fe; border-left: 4px solid #2196F3; padding: 10px; margin: 20px 0;\">\n        <p style=\"color: #000; line-height: 1.6; text-align: justify;\">\n    We have a considerable skewness in revol.bal, delinq.2yrs and pub.rec which shows they have outliers.\n         </p>","metadata":{}},{"cell_type":"code","source":"df_heatmap = data.drop(['purpose'], axis=1)\nplt.figure(figsize=(15,10))\nsns.heatmap(df_numeric.corr(), annot= True, cmap = 'crest')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #e7f3fe; border-left: 4px solid #2196F3; padding: 10px; margin: 20px 0;\">\n        <p style=\"color: #000; line-height: 1.6; text-align: justify;\">\nObservations:\nTarget feature 'not.fully.paid' has very week relation with following features:\n'delinq.2yrs','log.annual.inc','dti','days.with.cr.line' These Features can be removed from dataset.\n         </p>","metadata":{}},{"cell_type":"code","source":"# Create plots showing the uncertainity in the data and the outliers.\n\n# Define subplot grid\nfig, axs = plt.subplots(nrows=4, ncols=3, figsize=(15, 12), sharex=True)\nfig.subplots_adjust(hspace=0.5)\n\n# Loop through each column in df_numeric\nfor i, col in enumerate(df_numeric.columns):\n    ax = axs[i // 3, i % 3]\n    sns.boxplot(y=data[col], ax=ax)\n    ax.set_title(f\"Boxplot for {col}\")\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #e7f3fe; border-left: 4px solid #2196F3; padding: 10px; margin: 20px 0;\">\n        <p style=\"color: #000; line-height: 1.6; text-align: justify;\">\nFrom the graphs above, it can be seen that the outliers exist in the variables. These outliers will be handled later.\n         </p>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #ffa200; padding: 10px; margin: 20px 0;\">\n    <p style=\"color: #000; line-height: 1.6; text-align: justify; font-weight: bold; font-size: 18px;\">\n        Plotting the count plot for our categorical columns\n    </p>\n</div>","metadata":{}},{"cell_type":"code","source":"sns.set(style=\"darkgrid\")  \nplt.figure(figsize=(11,7))\nsns.countplot(x='purpose',hue='not.fully.paid',data=data,palette='Paired')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(style=\"darkgrid\")\nplt.figure(figsize=(8, 6))\nsns.countplot(x='not.fully.paid', data=data, palette='Set2')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the data\nsizes = data['credit.policy'].value_counts()\nlabels = sizes.index.tolist()\n\n# Define the color palette\ncolors = sns.color_palette('pastel')\n\n# Plot the pie chart with the defined color palette\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%')\nplt.title('dataframe by Credit Policy')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"section2\" style=\"background:linear-gradient(90deg, #004d40, #00796b); padding: 20px; border-radius: 10px;\">\n    <h2 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 32px; text-shadow: 2px 2px 4px rgba(0,0,0,0.2); text-align: center; margin-bottom: 50px\">\n        Data Preprocessing 🧹\n    </h2>\n    <ul style=\"list-style-type: none; padding: 0; font-family: 'Arial', sans-serif; font-size: 20px;\">\n        <li style=\"margin: 20px 0;\">\n            <a href=\"#missing\" style=\"text-decoration: none; color: #fff; background: #0b0c10; padding: 20px 15px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); display: inline-block; width: 100%; color: orange\">\n                - Missing Values 🕵️‍♂️\n            </a>\n        </li>\n        <li style=\"margin: 20px 0;\">\n            <a href=\"#outliers\" style=\"text-decoration: none; color: #fff; background: #0b0c10; padding: 20px 15px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); display: inline-block; width: 100%; color: orange\">\n                - Data Outliers 📊\n            </a>\n        </li>\n        <li style=\"margin: 20px 0;\">\n            <a href=\"#categorical\" style=\"text-decoration: none; color: #fff; background: #0b0c10; padding: 20px 15px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); display: inline-block; width: 100%; color: orange\">\n                - Handling Categorical Values 📬\n            </a>\n        </li>\n        <li style=\"margin: 20px 0;\">\n            <a href=\"#balancing\" style=\"text-decoration: none; color: #fff; background: #0b0c10; padding: 20px 15px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); display: inline-block; width: 100%; color: orange\">\n              - Balancing Data ⚖️  \n            </a>\n        </li>\n        <li style=\"margin: 20px 0;\">\n            <a href=\"#scaling\" style=\"text-decoration: none; color: #fff; background: #0b0c10; padding: 20px 15px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); display: inline-block; width: 100%; color: orange\">\n               - Data Scaling 📏 \n            </a>\n        </li>\n    </ul>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div id=\"missing\" style=\"background: linear-gradient(90deg, #4b79a1, #283e51); padding: 20px; text-align: center; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px;\">\n    <h1 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 36px; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n        Missing Values\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"data.count()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['not.fully.paid'].value_counts()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #f4f4f9; padding: 20px; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px; font-family: 'Arial', sans-serif;\">\n    <h2 style=\"color: #000;\">Overview</h2>\n    <p style=\"color: #555; line-height: 1.6; text-align: justify;\">\n        In this project, we analyzed a dataset containing various information related to loans. One crucial aspect of data analysis is the examination and identification of <em>Null</em> or <em>Missing</em> values. These values can significantly impact the results of analyses and machine learning models.\n    </p>\n    <div style=\"background-color: #e7f3fe; border-left: 4px solid #2196F3; padding: 10px; margin: 20px 0;\">\n        <p style=\"color: #555; line-height: 1.6; text-align: justify;\">\n            After thoroughly examining the dataset, it was determined that there are no <em>Null</em> or <em>Missing</em> values present. This allows us to proceed directly to the next steps of data processing and implementing machine learning algorithms without needing to impute or remove any data.\n        </p>\n    </div>\n    <p style=\"color: #555; line-height: 1.6; text-align: justify;\">\n        In the next stage, we will convert the <em>Categorical</em> columns in the dataset to numerical data so that they can be utilized in various models. The <strong>purpose</strong> column, which contains information about the loan's purpose, is one of these columns that will be transformed using <em>One-Hot Encoding</em>.\n    </p>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div id=\"outliers\" style=\"background: linear-gradient(90deg, #4b79a1, #283e51); padding: 20px; text-align: center; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px;\">\n    <h1 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 36px; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n        Data Outliers\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"X = data.drop(columns=['not.fully.paid'])\ny = data.iloc[:, -1]\n\nprint(f'X values: {X} \\n\\n==========================================================================\\n\\nY values (target): {y}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_to_plot = [\n    'int.rate', 'installment', 'log.annual.inc', 'dti', 'fico',\n    'days.with.cr.line', 'revol.bal', 'revol.util', 'inq.last.6mths',\n    'delinq.2yrs', 'pub.rec'\n]\n\nplt.figure(figsize=(20, 20))\nsns.set(style=\"whitegrid\")\n\nfor i, column in enumerate(columns_to_plot, 1):\n    plt.subplot(4, 3, i)\n    sns.boxplot(y=X[column])\n    plt.title(column)\n    \nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_to_check = [\n    'int.rate', 'installment', 'log.annual.inc', 'dti', 'fico',\n    'days.with.cr.line', 'revol.bal', 'revol.util', 'inq.last.6mths',\n    'delinq.2yrs', 'pub.rec'\n]\n\ndef cap_outliers(df, columns):\n    for column in columns:\n        Q1 = df[column].quantile(0.25)\n        Q3 = df[column].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n        df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n    return df\n\nX_cleaned = cap_outliers(X.copy(), columns_to_check)\n\nplt.figure(figsize=(20, 20))\nsns.set(style=\"whitegrid\")\n\nfor i, column in enumerate(columns_to_check, 1):\n    plt.subplot(4, 3, i)\n    sns.boxplot(y=X_cleaned[column])\n    plt.title(column)\n    \nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_cleaned.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #f4f4f9; padding: 20px; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px; font-family: 'Arial', sans-serif;\">\n    <h2 style=\"color: #333;\">Outlier Detection and Handling</h2>\n    <p style=\"color: #555; line-height: 1.6; text-align: justify;\">\n        In this project, we carefully analyzed the dataset to ensure the quality and reliability of our data. One crucial step in this process is identifying and managing <em>Outliers</em>. Outliers are data points that deviate significantly from the rest of the dataset and can skew analysis results, potentially leading to incorrect conclusions and less accurate models.\n    </p>\n    <div style=\"background-color: #e7f3fe; border-left: 4px solid #2196F3; padding: 10px; margin: 20px 0;\">\n        <p style=\"color: #555; line-height: 1.6; text-align: justify;\">\n            After plotting boxplots for several columns in the dataset, it became evident that some columns had significant outliers. These columns included <strong>int.rate</strong>, <strong>installment</strong>, <strong>log.annual.inc</strong>, <strong>dti</strong>, <strong>fico</strong>, <strong>days.with.cr.line</strong>, <strong>revol.bal</strong>, <strong>revol.util</strong>, <strong>inq.last.6mths</strong>, <strong>delinq.2yrs</strong>, and <strong>pub.rec</strong>. Outliers in these columns could adversely affect the performance of our machine learning models.\n        </p>\n    </div>\n    <div style=\"background-color: #fff3cd; border-left: 4px solid #ffecb5; padding: 10px; margin: 20px 0;\">\n        <p style=\"color: #555; line-height: 1.6; text-align: justify;\">\n                    To address this issue, we employed the <strong>Interquartile Range (IQR)</strong> method to detect and remove outliers. The IQR method is a robust technique that identifies outliers as data points that fall below the first quartile (Q1) minus 1.5 times the IQR or above the third quartile (Q3) plus 1.5 times the IQR. By applying this method, we successfully cleaned the dataset of outliers, ensuring a more accurate and reliable dataset for subsequent analysis and modeling.\n        </p>\n    </div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div id=\"categorical\" style=\"background: linear-gradient(90deg, #4b79a1, #283e51); padding: 20px; text-align: center; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px;\">\n    <h1 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 36px; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n        Categorical Valeus\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Perform one-hot encoding for the 'purpose' column\n\ndf_encoded = pd.get_dummies(data, columns=['purpose'])\ndf_encoded = df_encoded.astype(int)\n\n# Display the resulting DataFrame with one-hot encoded columns\ndf_encoded.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_encoded.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the common column names between encoded df and numeric cleaned df\ncommon_columns = df_encoded.columns.intersection(X_cleaned.columns)\n\n# merge these two data frames\ndf_encoded[common_columns] = X_cleaned[common_columns]\ndf1 = df_encoded\ndf1.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #f4f4f9; padding: 20px; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px; font-family: 'Arial', sans-serif;\">\n    <h2 style=\"color: #333;\">Categorical Data Encoding</h2>\n    <p style=\"color: #555; line-height: 1.6; text-align: justify;\">\n        In our dataset, the <em>purpose</em> column contains categorical data representing the reason for the loan application. Machine learning algorithms require numerical input, so it's essential to transform categorical variables into numerical formats to be effectively used in model training.\n    </p>\n    <div style=\"background-color: #e7f3fe; border-left: 4px solid #2196F3; padding: 10px; margin: 20px 0;\">\n        <p style=\"color: #555; line-height: 1.6; text-align: justify;\">\n            To achieve this, we applied <strong>One-Hot Encoding</strong> to the <em>purpose</em> column. One-Hot Encoding converts categorical variables into a series of binary columns, each representing a unique category. This technique avoids introducing ordinal relationships between the categories, which might otherwise mislead the learning algorithms.\n        </p>\n    </div>\n    <div style=\"background-color: #FADADD; border-left: 4px solid #2196F3; padding: 10px; margin: 20px 0;\">\n    <p style=\"color: #555; line-height: 1.6; text-align: justify;\">\n        The transformation was successfully completed, converting the <em>purpose</em> column into multiple binary columns. This ensures that the categorical information is effectively represented in a numerical format, allowing us to proceed with the machine learning pipeline. With the dataset now containing only numerical values, we can confidently apply various algorithms to analyze and model the loan data.\n    </p>\n    </div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div id=\"balancing\" style=\"background: linear-gradient(90deg, #4b79a1, #283e51); padding: 20px; text-align: center; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px;\">\n    <h1 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 36px; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n        Balancing\n    </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"y.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smk = SMOTETomek()\n\nX = df1.drop('not.fully.paid',axis=1)\ny = df1['not.fully.paid']\n\nX_res,y_res=smk.fit_resample(X,y)\n\ny_res.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"scaling\" style=\"background: linear-gradient(90deg, #4b79a1, #283e51); padding: 20px; text-align: center; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px;\">\n    <h1 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 36px; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n        Scaling Values\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"ss = StandardScaler()\nX_res_scaled = ss.fit_transform(X_res)\nX_res_scaled.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #f4f4f9; padding: 20px; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px; font-family: 'Arial', sans-serif;\">\n    <h2 style=\"color: #333;\">Scaling Numerical Data</h2>\n    <p style=\"color: #555; line-height: 1.6; text-align: justify;\">\n        As part of the data preprocessing phase, scaling numerical data is a crucial step to ensure that all features contribute equally to the machine learning algorithm's training process. Scaling helps in standardizing the range of independent variables or features of the dataset, which is particularly important for algorithms that rely on distance measures, such as k-nearest neighbors and support vector machines.\n    </p>\n    <div style=\"background-color: #e7f3fe; border-left: 4px solid #2196F3; padding: 10px; margin: 20px 0;\">\n        <p style=\"color: #555; line-height: 1.6; text-align: justify;\">\n            In our dataset, features like <strong>int.rate</strong>, <strong>installment</strong>, <strong>log.annual.inc</strong>, <strong>dti</strong>, <strong>fico</strong>, <strong>days.with.cr.line</strong>, <strong>revol.bal</strong>, and <strong>revol.util</strong> are numerical and may have different scales. To address this issue, we applied the <strong>StandardScaler</strong> technique. StandardScaler standardizes the numerical features by removing the mean and scaling to unit variance. This transformation ensures that the features have a mean of 0 and a standard deviation of 1.\n        </p>\n    </div>\n    <p style=\"color: #555; line-height: 1.6; text-align: justify;\">\n        By performing StandardScaler on these numerical features, we standardized their distributions, making them comparable and preventing features with larger scales from dominating those with smaller scales during model training. This process facilitates the convergence of algorithms, improves the performance of the models, and enhances their interpretability.\n    </p>\n</div>","metadata":{}},{"cell_type":"code","source":"features_to_drop = ['delinq.2yrs', 'log.annual.inc', 'dti', 'days.with.cr.line']\n# Drop the specified features\ndf2 = df1.drop(columns=features_to_drop)\ndf2.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We made df2 by dropping the columns that has min correlation with target but the f1score decreases a bit...so df2 is not used","metadata":{}},{"cell_type":"markdown","source":"<div id=\"section3\" style=\"background:linear-gradient(90deg, #7AB2B2, #9DDE8B); padding: 20px; border-radius: 10px;\">\n    <h2 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 32px; text-shadow: 2px 2px 4px rgba(0,0,0,0.2); text-align: center; margin-bottom: 50px\">\n        Machine Leaning Models 🤖\n    </h2>\n<ul style=\"list-style-type: none; padding: 0; font-family: 'Arial', sans-serif; font-size: 20px;\">\n    <li style=\"margin: 20px 0;\">\n            <a href=\"#dct\" style=\"text-decoration: none; color: #fff; background: #AD88C6; padding: 20px 15px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); display: inline-block; width: 100%; color: white\">\n                - Decision Tree\n            </a>\n        </li>\n        <li style=\"margin: 20px 0;\">\n            <a href=\"#svm\" style=\"text-decoration: none; color: #fff; background: #AD88C6; padding: 20px 15px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); display: inline-block; width: 100%; color: white\">\n                - SVM\n            </a>\n        </li>\n        <li style=\"margin: 20px 0;\">\n            <a href=\"#knn\" style=\"text-decoration: none; color: #fff; background: #AD88C6; padding: 20px 15px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); display: inline-block; width: 100%; color: white\">\n                - KNN\n            </a>\n        </li>\n     <li style=\"margin: 20px 0;\">\n            <a href=\"#rf\" style=\"text-decoration: none; color: #fff; background: #AD88C6; padding: 20px 15px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); display: inline-block; width: 100%; color: white\">\n                - Random Forest\n            </a>\n        </li>\n     <li style=\"margin: 20px 0;\">\n            <a href=\"#cb\" style=\"text-decoration: none; color: #fff; background: #AD88C6; padding: 20px 15px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); display: inline-block; width: 100%; color: white\">\n                - CatBoost\n            </a>\n        </li>\n    </ul>\n</div>","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_res_scaled, y_res, random_state = 100, test_size=0.30)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"dct\" style=\"background: linear-gradient(90deg, #4b79a1, #283e51); padding: 20px; text-align: center; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px;\">\n    <h1 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 36px; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n        Decision Tree\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"dtree_clf = DecisionTreeClassifier()\ndtree_clf.fit(X_train, y_train)\n\ny_pred_dtree = dtree_clf.predict(X_test)\nprint(metrics.classification_report(y_test, y_pred_dtree))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(confusion_matrix(y_test, y_pred_dtree))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"cv\" style=\"background: linear-gradient(90deg, #4b79a1, #283e51); padding: 20px; text-align: center; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px;\">\n    <h1 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 24px; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n        Implementing K-Fold Cross-Validation for Model Evaluation (Decision Tree)\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=1)\n\n# Perform cross-validation and get f1-scores for each fold\nf1_scores = cross_val_score(dtree_clf, X_res_scaled, y_res, cv=kf, scoring='f1')\n\n# Print the results of cross-validation\nprint(f\"F1-scores for each fold: {f1_scores}\")\nprint(f\"Mean F1-score: {np.mean(f1_scores)}\")\nprint(f\"Standard Deviation of F1-scores: {np.std(f1_scores)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"cv\" style=\"background: linear-gradient(90deg, #4b79a1, #283e51); padding: 20px; text-align: center; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px;\">\n    <h1 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 32px; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n        Randomized Search CV on Decision Tree\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Define the parameter grid\nparam_grid = {\n    'criterion': ['gini', 'entropy'],\n    'splitter': ['best', 'random'],\n    'max_depth': [None, 10, 20, 30, 40, 50],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': [None, 'auto', 'sqrt', 'log2']\n}\n\n# Initialize the Randomized Search CV\nrandom_search = RandomizedSearchCV(estimator=dtree_clf, param_distributions=param_grid, \n                                   n_iter=150, cv=kf, verbose=1, random_state=1, n_jobs=-1, scoring='f1')\n\n# Fit the Randomized Search CV\nrandom_search.fit(X_res_scaled, y_res)\n\n# Print the best parameters found by Randomized Search CV\nprint(f\"Best parameters found: {random_search.best_params_}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model with the best found parameters on the test set\nbest_dtree = random_search.best_estimator_\npredictions = best_dtree.predict(X_test)\n\n# Print classification report and confusion matrix\nprint(classification_report(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the f1score jumps here, we should check whether it is because of the overfitting or not...so let's draw learning curve;","metadata":{}},{"cell_type":"markdown","source":"<div id=\"cv\" style=\"background: linear-gradient(90deg, #4b79a1, #283e51); padding: 20px; text-align: center; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px;\">\n    <h1 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 32px; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n        Learning Curves\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Function to plot learning curves\ndef plot_learning_curves(estimator, X, y):\n    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=kf, scoring='f1', n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n    \n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    \n    plt.figure()\n    plt.title(\"Learning Curves\")\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    plt.grid()\n    \n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n    \n    plt.legend(loc=\"best\")\n    return plt\n\n# Plot learning curves\nplot_learning_curves(random_search.best_estimator_, X_train, y_train)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If the model performs significantly better on the training data compared to the validation data, it is an indication of overfitting. so here we can observe overfitting and our first results are more reliable.","metadata":{}},{"cell_type":"markdown","source":"<div id=\"svm\" style=\"background: linear-gradient(90deg, #4b79a1, #283e51); padding: 20px; text-align: center; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px;\">\n    <h1 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 36px; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n        SVM (Support Vector Machine)\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"svm_clf = SVC(C=0.5, kernel='linear', random_state=100)\nsvm_clf.fit(X_train, y_train)\ny_pred_svm = svm_clf.predict(X_test)\nprint(metrics.classification_report(y_test, y_pred_svm))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"svm\" style=\"background: linear-gradient(90deg, #4b79a1, #283e51); padding: 20px; text-align: center; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px;\">\n    <h1 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 36px; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n        Grid Search on SVM\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Grid Search\n# Define the parameter grid\nparam_grid = {\n    'C': [0.1, 1, 10, 100],\n    'gamma': [1, 0.1, 0.01, 0.001],\n    'kernel': ['linear','rbf','poly']\n}\n# Initialize the Grid Search CV\ngrid_search = GridSearchCV(estimator=svm_clf, param_grid=param_grid, \n                           cv=kf, verbose=1, n_jobs=-1, scoring='f1')\n\n# Fit the Grid Search CV\ngrid_search.fit(X_train, y_train)\n\n# Print the best parameters found by Grid Search CV\nprint(f\"Best parameters found: {grid_search.best_params_}\")\n\n# Predict on the test set\ny_pred = grid_search.best_estimator_.predict(X_test)\n\n# Print classification report and confusion matrix\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the parameter grid for Randomized Search\nparam_distributions = {\n    'C': uniform(0.1, 10),\n    'gamma': uniform(0.01, 1),\n    'kernel': ['rbf', 'poly', 'linear']\n}\n# Initialize the Randomized Search CV\nrandom_search = RandomizedSearchCV(estimator=svm_clf, param_distributions=param_distributions, \n                                   n_iter=20, cv=kf, verbose=1, n_jobs=-1, scoring='f1')\n\n# Fit the Randomized Search CV\nrandom_search.fit(X_train, y_train)\n\n# Print the best parameters found by Randomized Search CV\nprint(f\"Best parameters found: {random_search.best_params_}\")\n\n# Make predictions on the test set using the best estimator\ny_pred = random_search.best_estimator_.predict(X_test)\n\n# Print classification report and confusion matrix\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It took a long long time to perform randomized search and specially grid search for svm as it has heavy computations for our processors; we just want to test it once and get familiar with it but the run didn't finish (we had to interrupt it).","metadata":{}},{"cell_type":"markdown","source":"<div id=\"knn\" style=\"background: linear-gradient(90deg, #4b79a1, #283e51); padding: 20px; text-align: center; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px;\">\n    <h1 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 36px; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n        KNN (K Nearest Neighbor)\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"knn_clf = KNeighborsClassifier(n_neighbors=5)\nknn_clf.fit(X_train, y_train)\n\ny_pred_knn = knn_clf.predict(X_test)\nprint(metrics.classification_report(y_test, y_pred_knn))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"rf\" style=\"background: linear-gradient(90deg, #4b79a1, #283e51); padding: 20px; text-align: center; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px;\">\n    <h1 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 36px; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n        Random Forest\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"rfc_clf = RandomForestClassifier(n_estimators=800, bootstrap=True, oob_score=True)\nrfc_clf.fit(X_train, y_train)\n\ny_pred_rfc = rfc_clf.predict(X_test)\nprint(metrics.classification_report(y_test, y_pred_rfc))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"cb\" style=\"background: linear-gradient(90deg, #4b79a1, #283e51); padding: 20px; text-align: center; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px;\">\n    <h1 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 36px; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n        CatBoost\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"catboost_clf = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, loss_function='MultiClass')\n\n# Train the classifier\ncatboost_clf.fit(X_train, y_train, verbose_eval=False)\n\n# Predict on the test set\ny_pred_cb = catboost_clf.predict(X_test)\n\nprint(classification_report(y_test, y_pred_cb))\n\nprint(confusion_matrix(y_test, y_pred_cb))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"section4\" style=\"background:linear-gradient(90deg, #7AB2B2, #9DDE8B); padding: 20px; border-radius: 10px;\">\n    <h2 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 32px; text-shadow: 2px 2px 4px rgba(0,0,0,0.2); text-align: center; margin-bottom: 50px\">\n        Deep Learning Models 🧠\n    </h2>\n    <ul style=\"list-style-type: none; padding: 0; font-family: 'Arial', sans-serif; font-size: 20px;\">\n        <li style=\"margin: 20px 0;\">\n            <a href=\"#dnn\" style=\"text-decoration: none; color: #fff; background: #AD88C6; padding: 20px 15px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); display: inline-block; width: 100%; color: white\">\n                - Dense Neural Network\n            </a>\n        </li>\n        <li style=\"margin: 20px 0;\">\n            <a href=\"#cnn\" style=\"text-decoration: none; color: #fff; background: #AD88C6; padding: 20px 15px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); display: inline-block; width: 100%; color: white\">\n                - CNN\n            </a>\n        </li>\n        <li style=\"margin: 20px 0;\">\n            <a href=\"#resnet\" style=\"text-decoration: none; color: #fff; background: #AD88C6; padding: 20px 15px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); display: inline-block; width: 100%; color: white\">\n                - ResNet\n            </a>\n        </li>\n    </ul>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div id=\"dnn\" style=\"background: linear-gradient(90deg, #4b79a1, #283e51); padding: 20px; text-align: center; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px;\">\n    <h1 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 36px; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n        Dense Neural Network (DNN)\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"modelNN = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(19, activation='relu', input_shape=(19,)),\n    tf.keras.layers.Dense(30, activation='relu'),\n    tf.keras.layers.Dense(60, activation='relu'),\n    tf.keras.layers.Dropout(0.3), \n    tf.keras.layers.Dense(80, activation='relu'),\n    tf.keras.layers.Dropout(0.3),  \n    tf.keras.layers.Dense(40, activation='relu'),\n    tf.keras.layers.Dense(25, activation='relu'),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])\n\nmodelNN.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=tf.keras.losses.BinaryCrossentropy(), \n              metrics=[tf.keras.metrics.BinaryAccuracy()])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelNN.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\nhistory = modelNN.fit(\n    X_train, y_train,\n    validation_data=(X_test, y_test),\n    batch_size=100,\n    epochs=300,\n    callbacks=[early_stopping],\n    verbose=1,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_NN = modelNN.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_NN","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_NN[y_pred_NN > 0.50] = 1\ny_pred_NN[y_pred_NN <= 0.50] = 0\ny_pred_NN","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_true=y_test, y_pred=y_pred_NN))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"cnn\" style=\"background: linear-gradient(90deg, #4b79a1, #283e51); padding: 20px; text-align: center; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px;\">\n    <h1 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 36px; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n        Convolutional Neural Network (CNN)\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# CNN\nmodelCNN = tf.keras.models.Sequential([\n    tf.keras.layers.Reshape((19, 1), input_shape=(19,)), \n    tf.keras.layers.Conv1D(32, 3, activation='relu', padding='same'),\n    #   tf.keras.layers.MaxPooling1D(2), NO maxpooling cause the datasize is small(=19*1)\n    tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same'),\n    tf.keras.layers.Conv1D(128, 3, activation='relu', padding='same'),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification output\n])\n\nmodelCNN.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br />\n<br />\n<h2 style=\"text-align: center;\">How does a 1D CNN work?</h2>\n<img src=\"CNN1D.png\" alt=\"How does a 1D CNN work?\" width=\"500\">","metadata":{}},{"cell_type":"code","source":"modelCNN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"es = EarlyStopping(monitor='val_loss', mode='min', patience=20)\nrlrop = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.2, min_lr=0.01)\nmch = ModelCheckpoint('Loan.keras', monitor='val_loss', mode='min', save_best_only=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = modelCNN.fit(X_train, y_train, epochs=300, validation_data=(X_test, y_test), callbacks=[es, rlrop, mch])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_CNN = modelCNN.predict(X_test)\n\n# Convert predicted probabilities to binary predictions\ny_pred_binary = (y_pred_CNN > 0.5).astype(int)\n\n# Calculate F1 score\nf1score = f1_score(y_test, y_pred_binary)\n\nprint(classification_report(y_true=y_test, y_pred=y_pred_binary))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background: linear-gradient(90deg, #4b79a1, #283e51); padding: 20px; text-align: center; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px;\">\n    <h1 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 36px; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n        CNN with LeakyReLu\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"modelCNN1 = tf.keras.models.Sequential([\n    tf.keras.layers.Reshape((19, 1), input_shape=(19,)), \n    tf.keras.layers.Conv1D(32, 3, activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding='same'),\n    # No maxpooling because the datasize is small (19*1)\n    tf.keras.layers.Conv1D(64, 3, activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding='same'),\n    tf.keras.layers.Conv1D(128, 3, activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding='same'),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification output\n])\nmodelCNN1.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelCNN1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"es = EarlyStopping(monitor='val_loss', mode='min', patience=20)\nrlrop = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.2, min_lr=0.01)\nmch = ModelCheckpoint('Loan.keras', monitor='val_loss', mode='min', save_best_only=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = modelCNN1.fit(X_train, y_train, epochs=300, validation_data=(X_test, y_test), callbacks=[es, rlrop, mch])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_CNN1 = modelCNN1.predict(X_test)\n\n# Convert predicted probabilities to binary predictions\ny_pred_binary1 = (y_pred_CNN1 > 0.5).astype(int)\n\n# Calculate F1 score\nf1score = f1_score(y_test, y_pred_binary1)\n\nprint(classification_report(y_true=y_test, y_pred=y_pred_binary1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"resnet\" style=\"background: linear-gradient(90deg, #4b79a1, #283e51); padding: 20px; text-align: center; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px;\">\n    <h1 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 36px; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n        ResNet\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"code","source":"#  ResNet\ndef residual_block(x, filters, kernel_size, dilation_rate):\n    \"\"\"Residual block with dilated convolution.\"\"\"\n    r = layers.Conv1D(filters, kernel_size, padding='same', dilation_rate=dilation_rate, activation='relu')(x)\n    r = layers.Conv1D(filters, kernel_size, padding='same', dilation_rate=dilation_rate, activation='relu')(r)\n    if x.shape[-1] != filters:\n        x = layers.Conv1D(filters, 1, padding='same')(x)\n    return layers.Add()([x, r])\n\ndef build_resnet(input_shape, num_classes):\n    \"\"\"Build a ResNet model for 1D data.\"\"\"\n    inputs = tf.keras.Input(shape=input_shape)\n    \n    # Initial convolutional layer\n    x = layers.Conv1D(64, 3, padding='same', activation='relu')(inputs)\n    \n    # Residual blocks\n    x = residual_block(x, 64, 3, 1)\n    x = residual_block(x, 64, 3, 1)\n    \n    # Global average pooling\n    x = layers.GlobalAveragePooling1D()(x)\n    \n    # Fully connected layer\n    x = layers.Dense(128, activation='relu')(x)\n    x = layers.Dropout(0.5)(x)\n    \n    # Output layer\n    outputs = layers.Dense(num_classes, activation='sigmoid')(x)\n    \n    model = tf.keras.Model(inputs, outputs)\n    return model\n\n# Example usage\ninput_shape = (19, 1)  # Assuming input text sequences of length 100\nnum_classes = 1  # Number of output classes (binary classification)\nmodelresnet = build_resnet(input_shape, num_classes)\nmodelresnet.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile the model\nmodelresnet.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nhistory = modelresnet.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n\n# Plot training history\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The validation plot doesn't show a good proceeding...let's make some change;(Learning rate changing over an interval)","metadata":{}},{"cell_type":"code","source":"# Define optimizer and compile the model\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)  # Initial learning rate\nmodelresnet.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\n# Define learning rate scheduler\nlr_scheduler = callbacks.LearningRateScheduler(lambda epoch: 0.001 * (0.9 ** epoch))  # Exponential decay\n\n# Train the model\nhistory = modelresnet.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), callbacks=[lr_scheduler])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No improvements...We changed activation functions and it didn't change the accuracy significantly...We changed number of layers and other hyperparameters to reach the best we can...it seems that this model is not perfect for 1D data. for the last try we make the artchitecture a bit more complex.","metadata":{}},{"cell_type":"code","source":"# Define the Residual block function\ndef residual_block(x, filters, kernel_size, dilation_rate):\n    \"\"\"Residual block with dilated convolution.\"\"\"\n    r = layers.Conv1D(filters, kernel_size, padding='same', dilation_rate=dilation_rate, activation='relu')(x)\n    r = layers.Conv1D(filters, kernel_size, padding='same', dilation_rate=dilation_rate, activation='relu')(r)\n    if x.shape[-1] != filters:\n        x = layers.Conv1D(filters, 1, padding='same')(x)\n    return layers.Add()([x, r])\n\n# Build the ResNet model with LeakyReLU activation and more complexity\ndef build_resnet_complex(input_shape, num_classes):\n    \"\"\"Build a more complex ResNet model for 1D data with LeakyReLU activation.\"\"\"\n    inputs = tf.keras.Input(shape=input_shape)\n    \n    # Initial convolutional layer\n    x = layers.Conv1D(64, 3, padding='same', activation='relu')(inputs)\n    \n    # Residual blocks\n    x = residual_block(x, 64, 3, 1)\n    x = residual_block(x, 64, 3, 1)\n    x = residual_block(x, 128, 3, 2)  # Increase filters and dilation rate\n    x = residual_block(x, 128, 3, 2)\n    x = residual_block(x, 256, 3, 3)  # Increase filters and dilation rate\n    x = residual_block(x, 256, 3, 3)\n    \n    # Global average pooling\n    x = layers.GlobalAveragePooling1D()(x)\n    \n    # Fully connected layer\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.Dropout(0.5)(x)\n    \n    # Output layer\n    outputs = layers.Dense(num_classes, activation='sigmoid')(x)\n    \n    model = tf.keras.Model(inputs, outputs)\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_resnet_complex = build_resnet_complex(input_shape, num_classes)\n\n# Compile the model\nmodel_resnet_complex.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model_resnet_complex.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on test data\ntest_loss, test_accuracy = model_resnet_complex.evaluate(X_test, y_test)\nprint(f'Test Accuracy: {test_accuracy}')\n\n# Make predictions\npredictions = model_resnet_complex.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"section5\" style=\"background: linear-gradient(90deg, #4b79a1, #283e51); padding: 20px; text-align: center; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px;\">\n    <h1 style=\"color: white; font-family: 'Arial', sans-serif; font-size: 36px; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n        Conclusion\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #f4f4f9; padding: 20px; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin-bottom: 24px; font-family: 'Arial', sans-serif;\">\n    <h2 style=\"color: #333;\">Conclusion of the project🔚</h2>\n    <p style=\"color: #555; line-height: 1.6; text-align: justify;\">\n        The DeepLearning models needs large dataset and here we get better answers from machine learnning algorithms.Here are some reasons why:\n    </p>\n    <div style=\"background-color: #e7f3fe; border-left: 4px solid #2196F3; padding: 10px; margin: 20px 0;\">\n        <p style=\"color: #555; line-height: 1.6; text-align: justify;\">\n            Data Size and Quality: Deep learning models often require a large amount of data to generalize well. If you have a relatively small dataset, it may not provide enough examples for the deep learning model to learn complex patterns effectively. Additionally, if the data is noisy or contains outliers, it can negatively impact the performance of deep learning models.\n        </p>\n    </div>\n    <div style=\"background-color: #FADADD; border-left: 4px solid #2196F3; padding: 10px; margin: 20px 0;\">\n    <p style=\"color: #555; line-height: 1.6; text-align: justify;\">\n        Model Complexity and Hyperparameters: Deep learning models have many hyperparameters to tune, such as the number of layers, the number of neurons in each layer, learning rate, batch size, etc. Finding the optimal set of hyperparameters can be challenging and often requires extensive experimentation. In contrast, algorithms like random forest or KNN have fewer hyperparameters and may be easier to tune.\n    </p>\n    </div>\n    <div style=\"background-color: #fff3cd; border-left: 4px solid #2196F3; padding: 10px; margin: 20px 0;\">\n    <p style=\"color: #555; line-height: 1.6; text-align: justify;\">\n        Interpretability: Deep learning models are often considered \"black boxes\" because of their complex architectures and large number of parameters. In contrast, models like random forest or KNN are more interpretable, making it easier to understand and debug issues with the model.\n    </p>\n    </div>\n    <div style=\"background-color: #ffa200; border-left: 4px solid #2196F3; padding: 10px; margin: 20px 0;\">\n    <p style=\"color: #555; line-height: 1.6; text-align: justify;\">\n        Feature Engineering: Deep learning models are capable of automatically learning features from raw data. However, in some cases, carefully engineered features may provide better discrimination between classes, especially for smaller datasets. Random forest and KNN algorithms can work well with handcrafted features.\n    </p>\n    </div>\n</div>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}