{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6429459,"sourceType":"datasetVersion","datasetId":3709687}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/maryamkamelan/online-retail-unsupervisedlearning?scriptVersionId=204382214\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.metrics import silhouette_score\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-05-14T17:25:52.841495Z","iopub.execute_input":"2024-05-14T17:25:52.84186Z","iopub.status.idle":"2024-05-14T17:25:56.121726Z","shell.execute_reply.started":"2024-05-14T17:25:52.841821Z","shell.execute_reply":"2024-05-14T17:25:56.120497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_excel('/kaggle/input/online-retail/Online Retail.xlsx')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T17:25:58.224239Z","iopub.execute_input":"2024-05-14T17:25:58.224787Z","iopub.status.idle":"2024-05-14T17:27:12.998968Z","shell.execute_reply.started":"2024-05-14T17:25:58.224753Z","shell.execute_reply":"2024-05-14T17:27:12.997805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems we have null values in CustomerID column; so we have to omit them as they do not contain any important information.","metadata":{}},{"cell_type":"code","source":"data = data[data['CustomerID'].notnull()]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group transactions by CustomerID and aggregate StockCode into lists\ntransactions = data.groupby('CustomerID')['StockCode'].apply(list)\n\n# Convert transactions to lists of lists of strings\ntransactions_list = transactions.apply(lambda x: [str(i) for i in x]).tolist()\n\n# Use MultiLabelBinarizer to transform lists of lists into binary matrix\nmlb = MultiLabelBinarizer()\nbinary_matrix = mlb.fit_transform(transactions_list)\n\n# Create DataFrame from binary matrix\nstock_codes_by_customer = pd.DataFrame(binary_matrix, columns=mlb.classes_, index=transactions.index)\n\nstock_codes_by_customer.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform K-means clustering\nkmeans = KMeans(n_clusters=4, random_state=42)\ncluster_labels = kmeans.fit_predict(stock_codes_by_customer)\n\n# Add cluster labels to the DataFrame\nKmeansClustered = stock_codes_by_customer.copy()\nKmeansClustered['Cluster'] = cluster_labels\n\n# Display the DataFrame with cluster labels\nKmeansClustered.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform KNN Classification\n# Split the data into features (X) and target (y)\nX = KmeansClustered.drop(columns=['Cluster'])  # Features\ny = KmeansClustered['Cluster']  # Target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Initialize the KNN classifier\nknn = KNeighborsClassifier(n_neighbors=10)  # We can adjust the number of neighbors as needed\n\n# Train the classifier on the training data\nknn.fit(X_train, y_train)\n\n# Predict the labels for the test data\ny_pred = knn.predict(X_test.values)\n\n# Evaluate the accuracy of the classifier\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Calculate precision, recall, and F1-score\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1 = f1_score(y_test, y_pred, average='weighted')\n\n# Print the results\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the classification results are very poor; so let's find the optimal K for clustering and do it again. ","metadata":{}},{"cell_type":"code","source":"wcss = {}\nfor k in range(1, 20):\n    kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300)\n    kmeans.fit(stock_codes_by_customer)\n    wcss[k] = kmeans.inertia_\nsns.pointplot(x=list(wcss.keys()), y=list(wcss.values()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I choose k=6 as the elbow part of the plot.","metadata":{}},{"cell_type":"code","source":"# Perform K-means clustering by the optimum k=6\nkmeans = KMeans(n_clusters=6, random_state=42)\ncluster_labels = kmeans.fit_predict(stock_codes_by_customer)\n\n# Add cluster labels to the DataFrame\nKmeansClustered = stock_codes_by_customer.copy()\nKmeansClustered['Cluster'] = cluster_labels\n\n# Display the DataFrame with cluster labels\nKmeansClustered.head(20)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Performing the knn classification again with the new clustering:","metadata":{}},{"cell_type":"code","source":"# Split the data into features (X) and target (y)\nX1 = KmeansClustered.drop(columns=['Cluster'])  # Features\ny1 = KmeansClustered['Cluster']  # Target\n\n# Split the data into training and testing sets\nX1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42)\n\n# Initialize the KNN classifier\nknn = KNeighborsClassifier(n_neighbors=10)  \n# We can adjust the number of neighbors as needed but here changing the n does not make any considerable change in results\n\n# Train the classifier on the training data\nknn.fit(X1_train, y1_train)\n\n# Predict the labels for the test data\ny1_pred = knn.predict(X1_test.values)\n\n# Evaluate the accuracy of the classifier\naccuracy = accuracy_score(y1_test, y1_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Calculate precision, recall, and F1-score\nprecision = precision_score(y1_test, y1_pred, average='weighted')\nrecall = recall_score(y1_test, y1_pred, average='weighted')\nf1 = f1_score(y1_test, y1_pred, average='weighted')\n\n# Print the results\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"$$$ The results have improved slightly but are still far from the permissible value! let's perform DBscan too:","metadata":{}},{"cell_type":"code","source":"# Perform DBSCAN clustering\ndbscan = DBSCAN(eps=0.1, min_samples=5)  # Adjust eps and min_samples\ncluster_labels = dbscan.fit_predict(stock_codes_by_customer)\n\n# Add cluster labels to the DataFrame\nDBscanClustered = stock_codes_by_customer.copy()\nDBscanClustered['DBSCAN_Cluster'] = cluster_labels\n\n# Display the DataFrame with DBSCAN cluster labels\nDBscanClustered.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see clustering has detected almost all the data as outlier! Adjusting eps and min_samples does not change the result. ","metadata":{}},{"cell_type":"code","source":"# Split the data into features (X) and target (y)\nX2 = DBscanClustered.drop(columns=['DBSCAN_Cluster'])  # Features\ny2 = DBscanClustered['DBSCAN_Cluster']  # Target\n\n# Split the data into training and testing sets\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)\n\n# Initialize the KNN classifier\nknn = KNeighborsClassifier(n_neighbors=5) \n\n# Train the classifier on the training data\nknn.fit(X2_train, y2_train)\n\n# Predict the labels for the test data\ny2_pred = knn.predict(X2_test.values)\n\n# Evaluate the accuracy of the classifier\naccuracy = accuracy_score(y2_test, y2_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Calculate precision, recall, and F1-score\nprecision = precision_score(y2_test, y2_pred, average='weighted')\nrecall = recall_score(y2_test, y2_pred, average='weighted')\nf1 = f1_score(y2_test, y2_pred, average='weighted')\n\n# Print the results\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"$$$ We get a much better scores here but they are not reliable because the clustering has not performed well.\n\nNow let's get use of the 'Quantity' column and with method BoP, let's create a data frame that shows the number of products purchased in each column.","metadata":{}},{"cell_type":"code","source":"# Group by CustomerID and StockCode and calculate the sum of quantity for each stock for each customer\ncustomer_stock_quantity = data.groupby(['CustomerID', 'StockCode'])['Quantity'].sum().reset_index()\n\n# Pivot the DataFrame to create a new DataFrame with CustomerID as rows and StockCode as columns\npivot_table = customer_stock_quantity.pivot_table(index='CustomerID', columns='StockCode', values='Quantity', fill_value=0)\n\n# Print the new DataFrame\npivot_table.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pivot_table.columns = pivot_table.columns.astype(str)\n\n# Perform K-means clustering\nkmeans = KMeans(n_clusters=5, random_state=42)\ncluster_labels = kmeans.fit_predict(pivot_table)\n\n# Add cluster labels to the DataFrame\nKmeansClusteredSum = pivot_table.copy()\nKmeansClusteredSum['Cluster'] = cluster_labels\n\n# Display the DataFrame with cluster labels\nKmeansClusteredSum.head(50)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As it can be observed, all the data are grouped in one cluster! First, we check if the data frame is made correctly or not. For this purpose, we calculate and compare the values of a cell manually:","metadata":{}},{"cell_type":"code","source":"pivot_table.loc[17850.0, '85123A']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_id = 17850.0  # Replace with the desired customer ID\nstock_code = '85123A'  # Replace with the desired stock code\n\n# Filter the DataFrame for the specified customer ID and stock code in different invoice numbers\nfiltered_df = data[(data['CustomerID'] == customer_id) & (data['StockCode'] == stock_code)]\n\n# Group by invoice number and sum the quantities for each invoice\ninvoice_sums = filtered_df.groupby('InvoiceNo')['Quantity'].sum()\n\n# Count the number of different invoices\nnum_invoices = len(invoice_sums)\n\n# Calculate the total quantity across all invoices\ntotal_quantity = invoice_sums.sum()\n\n# Print the results\nprint(\"Number of different invoices:\", num_invoices)\nprint(\"Total quantity for customer ID {} and stock code {}: {}\".format(customer_id, stock_code, total_quantity))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that the dataframe has been created correctly. Now we may have to normalize the data as a preprocessing and perform clustering again.","metadata":{}},{"cell_type":"code","source":"# Initialize the Normalizer object\nnormalizer = Normalizer(norm='l2')\n\n# Apply L2 normalization scaling to the pivot table data\nscaled_data = normalizer.fit_transform(pivot_table)\n\n# Convert the scaled data back to a DataFrame\nscaled_df = pd.DataFrame(scaled_data, columns=pivot_table.columns, index=pivot_table.index)\n\n# Display the scaled DataFrame\nscaled_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaled_df.columns = scaled_df.columns.astype(str)\n\n# Perform K-means clustering....I test n by different numbers and n=5 is the best\nkmeans = KMeans(n_clusters=5, random_state=42)\ncluster_labels = kmeans.fit_predict(scaled_df)\n\n# Add cluster labels to the DataFrame\nKmeansClusteredSumScaled = scaled_df.copy()\nKmeansClusteredSumScaled['Cluster'] = cluster_labels\n\n# Display the DataFrame with cluster labels\nKmeansClusteredSumScaled.head(50)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into features (X) and target (y)\nX3 = KmeansClusteredSumScaled.drop(columns=['Cluster'])  # Features\ny3 = KmeansClusteredSumScaled['Cluster']  # Target\n\n# Split the data into training and testing sets\nX3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.2, random_state=42)\n\n# Initialize the KNN classifier\nknn = KNeighborsClassifier(n_neighbors=3)  # I adjusted the number of neighbors and find an almost best n\n\n# Train the classifier on the training data\nknn.fit(X3_train, y3_train)\n\n# Predict the labels for the test data\ny3_pred = knn.predict(X3_test.values)\n\n# Evaluate the accuracy of the classifier\naccuracy = accuracy_score(y3_test, y3_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Calculate precision, recall, and F1-score\nprecision = precision_score(y3_test, y3_pred, average='weighted')\nrecall = recall_score(y3_test, y3_pred, average='weighted')\nf1 = f1_score(y3_test, y3_pred, average='weighted')\n\n# Print the results\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results are improved but still far from the perfect classification. let's draw elbow curve and calculate silhouette score to find a more precised optimum k for the normalized dataframe.","metadata":{}},{"cell_type":"code","source":"# Define a range of values for k\nk_values = range(2, 30) \n\n# Initialize lists to store inertia and silhouette scores\ninertia = []\nsilhouette_scores = []\n\n# Iterate over each value of k\nfor k in k_values:\n    # Initialize KMeans with the current value of k\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    \n    # Fit KMeans to the data\n    kmeans.fit(scaled_df)\n    \n    # Calculate inertia (within-cluster sum of squares)\n    inertia.append(kmeans.inertia_)\n    \n    # Calculate silhouette score\n    silhouette_scores.append(silhouette_score(scaled_df, kmeans.labels_))\n\n# Plot the elbow curve\nplt.figure(figsize=(10, 5))\nplt.plot(k_values, inertia, marker='o')\nplt.title('Elbow Method for Optimal k')\nplt.xlabel('Number of clusters (k)')\nplt.ylabel('Inertia')\nplt.xticks(k_values)\nplt.grid(True)\nplt.show()\n\n# Plot the silhouette scores\nplt.figure(figsize=(10, 5))\nplt.plot(k_values, silhouette_scores, marker='o')\nplt.title('Silhouette Scores for Optimal k')\nplt.xlabel('Number of clusters (k)')\nplt.ylabel('Silhouette Score')\nplt.xticks(k_values)\nplt.grid(True)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I changed the k interval several times but i can not find the elbow part of the plot. so it does not help me in finding the optimum k and I chose the k for clustering manually in the previous code cells.\n\nIn the following, we will perform the PCA operation so that maybe we can get better results by reducing the dimensions of the problem.","metadata":{}},{"cell_type":"code","source":"# Apply PCA\npca = PCA()\npca.fit(scaled_df)\n\n# Analyze the explained variance ratio\nexplained_variance_ratio = pca.explained_variance_ratio_\ncumulative_explained_variance = explained_variance_ratio.cumsum()\n\n# Plot explained variance ratio to decide on the number of components\nimport matplotlib.pyplot as plt\n\nplt.plot(range(1, len(explained_variance_ratio) + 1), cumulative_explained_variance, marker='o', linestyle='-')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('Explained Variance Ratio')\nplt.grid(True)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Choosing the number of components based on the plot. I chose 1000 as it is properly more than 90%\nn_components = 1000\n\n# Apply PCA with the chosen number of components\npca = PCA(n_components=n_components)\npca_data = pca.fit_transform(scaled_df)\n\n# Convert the PCA transformed data into a DataFrame\npca_data_df = pd.DataFrame(pca_data)\npca_data_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform K-means clustering....I test n by different numbers and n=5 is the best\nkmeans = KMeans(n_clusters=5, random_state=42)\ncluster_labels = kmeans.fit_predict(pca_data_df)\n\n# Add cluster labels to the DataFrame\nKmeansClusteredPCA = pca_data_df.copy()\nKmeansClusteredPCA['Cluster'] = cluster_labels\n\n# Display the DataFrame with cluster labels\nKmeansClusteredPCA.head(50)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X4 = KmeansClusteredPCA.drop(columns=['Cluster'])  # Features\ny4 = KmeansClusteredPCA['Cluster']  # Target\n\nX4_train, X4_test, y4_train, y4_test = train_test_split(X4, y4, test_size=0.3, random_state=42)\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X4_train, y4_train)\ny4_pred = knn.predict(X4_test)\naccuracy = accuracy_score(y4_test, y4_pred)\nprecision = precision_score(y4_test, y4_pred, average='weighted')\nrecall = recall_score(y4_test, y4_pred, average='weighted')\nf1 = f1_score(y4_test, y4_pred, average='weighted')\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our result has not improved; maybe because we lose important data from omitted columns that we didn't consider a logical way to delete them.\n\nNow it is time to perform DBScan on the l2norm scaled data that we named it \"scaled_df\" before.","metadata":{}},{"cell_type":"code","source":"# Perform DBSCAN clustering\ndbscan = DBSCAN(eps=0.1, min_samples=5)  # Adjust eps and min_samples as needed\ncluster_labels2 = dbscan.fit_predict(scaled_df)\n\n# Add cluster labels to the DataFrame\nDBscanClustered2 = scaled_df.copy()\nDBscanClustered2['DBSCAN_Cluster'] = cluster_labels2\n\n# Display the DataFrame with DBSCAN cluster labels\nDBscanClustered2.head(100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again we get an awful result of noises...here we try to find optimum eps and min samples by silhouette score:","metadata":{}},{"cell_type":"code","source":"eps_values = np.arange(0.1, 1.0, 0.1)\nmin_samples_values = range(1, 11)\n\n# Initialize lists to store silhouette scores\nsilhouette_scores = []\n\n# Iterate over different values of eps and min_samples\nfor eps in eps_values:\n    for min_samples in min_samples_values:\n        # Initialize DBSCAN with current parameters\n        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n        \n        # Fit DBSCAN to your data\n        dbscan.fit(scaled_df)\n        \n        # Calculate silhouette score\n        silhouette_avg = silhouette_score(scaled_df, dbscan.labels_)\n        silhouette_scores.append((eps, min_samples, silhouette_avg))\n\n# Extract eps, min_samples, and silhouette scores\neps_values = [score[0] for score in silhouette_scores]\nmin_samples_values = [score[1] for score in silhouette_scores]\nsilhouette_scores = [score[2] for score in silhouette_scores]\n\n# Plot silhouette scores\nplt.figure(figsize=(10, 6))\nplt.scatter(eps_values, min_samples_values, c=silhouette_scores, cmap='viridis')\nplt.colorbar(label='Silhouette Score')\nplt.xlabel('Eps')\nplt.ylabel('Min Samples')\nplt.title('Silhouette Score for Different Values of Eps and Min Samples')\nplt.grid(True)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Chosen values of eps\neps_values = [0.1,0.2,0.3,0.4, 0.5,0.6]\n\n# Define the range of values for min_samples\nmin_samples_values = range(1, 11)\n\n# Initialize lists to store silhouette scores\nsilhouette_scores = []\n\n# Iterate over different values of eps and min_samples\nfor eps in eps_values:\n    for min_samples in min_samples_values:\n        # Initialize DBSCAN with current parameters\n        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n        \n        # Fit DBSCAN to your data\n        dbscan.fit(scaled_df)\n        \n        # Calculate silhouette score\n        silhouette_avg = silhouette_score(scaled_df, dbscan.labels_)\n        silhouette_scores.append((eps, min_samples, silhouette_avg))\n\n# Filter silhouette scores for each chosen eps value\nfor eps in eps_values:\n    # Extract silhouette scores for the current eps value\n    scores_for_eps = [score[2] for score in silhouette_scores if score[0] == eps]\n    \n    # Plot min_samples values vs silhouette scores for the current eps\n    plt.plot(min_samples_values, scores_for_eps, label=f'eps={eps}')\n\n# Add labels and legend\nplt.xlabel('Min Samples')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Score for Different Values of Min Samples for Chosen Eps')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As shown in the both diagrams, performing DBSCAN does not have a good results on the BoP method for this dataset.","metadata":{}},{"cell_type":"code","source":"X5 = DBscanClustered2.drop(columns=['DBSCAN_Cluster'])  # Features\ny5 = DBscanClustered2['DBSCAN_Cluster']  # Target\n\nX5_train, X5_test, y5_train, y5_test = train_test_split(X5, y5, test_size=0.3, random_state=42)\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X5_train, y5_train)\ny5_pred = knn.predict(X5_test)\naccuracy = accuracy_score(y5_test, y5_pred)\nprecision = precision_score(y5_test, y5_pred, average='weighted')\nrecall = recall_score(y5_test, y5_pred, average='weighted')\nf1 = f1_score(y5_test, y5_pred, average='weighted')\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As a last attempt, since our manipulated dataframe is a sparse matrix, we select a number of products that have the highest sales number. For choosing the number of kept columns, different values were tested and the best value was around 50.","metadata":{}},{"cell_type":"code","source":"# Calculate the sum of values for each column\n\nstock_codes_top_50 =[]\ncolumn_sums = pivot_table.sum(axis=0)\n\n# Sort the columns by their sum in descending order and select the top 50 columns\ntop_50_columns = column_sums.sort_values(ascending=False).head(50).index\n\n# Select only the top 50 columns\nstock_codes_top_50 = pivot_table[top_50_columns]\n\n# Display the DataFrame with only the top 50 columns\nstock_codes_top_50.head(50)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the Normalizer object\nnormalizer = Normalizer(norm='l2')\n\n# Apply L2 normalization scaling \nscaled_data50 = normalizer.fit_transform(stock_codes_top_50)\n\n# Convert the scaled data back to a DataFrame\nscaled_df50 = pd.DataFrame(scaled_data50, columns=stock_codes_top_50.columns, index=stock_codes_top_50.index)\n\n# Display the scaled DataFrame\nscaled_df50","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform K-means clustering\nkmeans = KMeans(n_clusters=5, random_state=42)\ncluster_labels50 = kmeans.fit_predict(scaled_df50)\n\n# Add cluster labels to the DataFrame\nKmeansClustered50 = scaled_df50.copy()\nKmeansClustered50['Cluster'] = cluster_labels50\n\n# Display the DataFrame with cluster labels\nKmeansClustered50.head(50)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X6 = KmeansClustered50.drop(columns=['Cluster'])  # Features\ny6 =KmeansClustered50['Cluster']  # Target\n\n# Split the data into training and testing sets\nX6_train, X6_test, y6_train, y6_test = train_test_split(X6, y6, test_size=0.2, random_state=42)\n\n# Initialize the KNN classifier\nknn = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors as needed\n\n# Train the classifier on the training data\nknn.fit(X6_train, y6_train)\n\n# Predict the labels for the test data\ny6_pred = knn.predict(X6_test.values)\n\n# Evaluate the accuracy of the classifier\naccuracy = accuracy_score(y6_test, y6_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Calculate precision, recall, and F1-score\nprecision = precision_score(y6_test, y6_pred, average='weighted')\nrecall = recall_score(y6_test, y6_pred, average='weighted')\nf1 = f1_score(y6_test, y6_pred, average='weighted')\n\n# Print the results\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform DBSCAN clustering\ndbscan = DBSCAN(eps=0.1, min_samples=4)  # Adjust eps and min_samples as needed\ncluster_labelsDB = dbscan.fit_predict(scaled_df50)\n\n# Add cluster labels to the DataFrame\nDBscanClustered50 = scaled_df50.copy()\nDBscanClustered50['DBSCAN_Cluster'] = cluster_labelsDB\n\n# Display the DataFrame with DBSCAN cluster labels\nDBscanClustered50.head(50)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"DBSCAN still detects so much noise in the dataset so the knn results can not be reliable.","metadata":{}},{"cell_type":"code","source":"X7 = DBscanClustered50.drop(columns=['DBSCAN_Cluster'])  # Features\ny7 =DBscanClustered50['DBSCAN_Cluster']  # Target\n\n# Split the data into training and testing sets\nX7_train, X7_test, y7_train, y7_test = train_test_split(X7, y7, test_size=0.2, random_state=42)\n\n# Initialize the KNN classifier\nknn = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors as needed\n\n# Train the classifier on the training data\nknn.fit(X7_train, y7_train)\n\n# Predict the labels for the test data\ny7_pred = knn.predict(X7_test.values)\n\n# Evaluate the accuracy of the classifier\naccuracy = accuracy_score(y7_test, y7_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Calculate precision, recall, and F1-score\nprecision = precision_score(y7_test, y7_pred, average='weighted')\nrecall = recall_score(y7_test, y7_pred, average='weighted')\nf1 = f1_score(y7_test, y7_pred, average='weighted')\n\n# Print the results\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"$$$ Let's add a new feature and calculate again...the new feature is the number of stocks each costumer has bought.","metadata":{}},{"cell_type":"code","source":"# Add a new column to the DataFrame that contains the sum of stock codes for each customer\nstock_codes_top_New = stock_codes_top_50.copy()\nstock_codes_top_New['Number_of_Stocks'] = stock_codes_top_50.sum(axis=1)\nstock_codes_top_New.head(100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the Normalizer object\nnormalizer = Normalizer(norm='l2')\n\n# Apply L2 normalization scaling \nscaled_dataNew = normalizer.fit_transform(stock_codes_top_New)\n\n# Convert the scaled data back to a DataFrame\nscaled_dfNew = pd.DataFrame(scaled_dataNew, columns=stock_codes_top_New.columns, index=stock_codes_top_New.index)\n\n# Display the scaled DataFrame\nscaled_dfNew.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform K-means clustering\nkmeans = KMeans(n_clusters=5, random_state=42)\ncluster_labelsNew = kmeans.fit_predict(scaled_dfNew)\n\n# Add cluster labels to the DataFrame\nKmeansClusteredNew = scaled_dfNew.copy()\nKmeansClusteredNew['Cluster'] = cluster_labelsNew\n\n# Display the DataFrame with cluster labels\nKmeansClusteredNew.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X8 = KmeansClusteredNew.drop(columns=['Cluster'])  # Features\ny8 =KmeansClusteredNew['Cluster']  # Target\n\n# Split the data into training and testing sets\nX8_train, X8_test, y8_train, y8_test = train_test_split(X8, y8, test_size=0.2, random_state=42)\n\n# Initialize the KNN classifier\nknn0 = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors as needed\n\n# Train the classifier on the training data\nknn0.fit(X8_train, y8_train)\n\n# Predict the labels for the test data\ny8_pred = knn0.predict(X8_test.values)\n\n# Evaluate the accuracy of the classifier\naccuracy = accuracy_score(y8_test, y8_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Calculate precision, recall, and F1-score\nprecision = precision_score(y8_test, y8_pred, average='weighted')\nrecall = recall_score(y8_test, y8_pred, average='weighted')\nf1 = f1_score(y8_test, y8_pred, average='weighted')\n\n# Print the results\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can conclude that the total number of products bought by a customer is not very important but how many product group is selected is significant.\n\nLet's check the min and max values of NumberOfStocks to become sure about the calculations:","metadata":{}},{"cell_type":"code","source":"# Find the minimum value of the column\nmin_value = stock_codes_top_New['Number_of_Stocks'].min()\n\n# Find the maximum value of the column\nmax_value = stock_codes_top_New['Number_of_Stocks'].max()\n\n# Print the results\nprint(\"Minimum value:\", min_value)\nprint(\"Maximum value:\", max_value)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we see that we have negative values in the stocks number. it is a fault in dataframe! we should check how many rows have a negative false value","metadata":{}},{"cell_type":"code","source":"negative_values = stock_codes_top_New[stock_codes_top_New['Number_of_Stocks'] < 0]\n\n# Print the rows with negative values\nprint(\"Rows with negative values:\")\nprint(negative_values)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is just 3 rows...so we can ignore them! :D","metadata":{}},{"cell_type":"markdown","source":"**Determine the Minimum Number of Products: \n\nthe minimum number of products needed to be bought by a new customer for accurate predictions depends on various factors such as the complexity of the data, the distribution of classes, the quality of features, and the nature of the problem. It's typically determined empirically through experimentation and validation.\n\nI have used two methods for it... ","metadata":{}},{"cell_type":"code","source":"# Define a function to prepare the dataset for a new customer with a specified number of products\n\ndef prepare_new_customer_data(num_products):\n    # Select the top `num_products` stock codes\n    top_products = KmeansClustered50.columns[:-1].values  # Exclude the last column (Cluster)\n    \n    # Create a new DataFrame for the new customer data\n    new_customer_data = pd.DataFrame(columns=top_products)\n    \n    # Fill the DataFrame with zeros (assuming the customer hasn't bought any products yet)\n    new_customer_data.loc[0] = np.zeros(len(top_products))\n     \n    # Assign a random quantity to each\n    selected_products = np.random.choice(top_products, size=num_products, replace=False)\n    for product in selected_products:\n        new_customer_data.loc[0, product] = np.random.randint(1, 11)  # Random quantity between 1 and 10\n    \n    return new_customer_data\n########################################################\n\n# Split the data into features (X) and labels (y)\nX = KmeansClustered50.drop(columns=['Cluster'])  # Features\ny = KmeansClustered50['Cluster']  # Target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Initialize the KNN classifier\nknn_model = KNeighborsClassifier()\n\n# Train the classifier on the training data\nknn_model.fit(X_train, y_train)\n\n###################################################\n\n# Define a range of values for the number of products\nnum_products_range = range(1, len(X.columns) + 1)  # Consider all possible products\n\nstable_predictions = False\nthreshold_accuracy = 0.95  # Threshold for stability\n\n# Iterate over the range of number of products\nfor num_products in num_products_range:\n    # Prepare dataset with the chosen number of products\n    new_customer_data = prepare_new_customer_data(num_products)\n    \n    # Predict the cluster for the new customer\n    predicted_cluster = knn_model.predict(new_customer_data)\n    predicted_cluster = np.repeat(predicted_cluster, len(y_test))  # Repeat the prediction for all test samples\n    \n    # Check if predictions are stable (i.e., accuracy is above the threshold)\n    accuracy = accuracy_score(y_test, predicted_cluster)\n    if accuracy >= threshold_accuracy:\n        # Stable predictions achieved, record the number of products\n        min_products_required = num_products\n        stable_predictions = True\n        break\n\n# If stable predictions were not achieved, set min_products_required to the maximum number of products\nif not stable_predictions:\n    min_products_required = len(KmeansClustered50.columns) - 1\n\nprint(\"Minimum number of products required:\", min_products_required)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this method I made a whole zero Xtest and then add non zero values to it step by step until the evaluation metrics(here accuracy) reach the initial state(initial state is when we use 30% of the main data for test).\n\n50 is the answer where stable predictions are not achieved. Let's examine another way;","metadata":{}},{"cell_type":"code","source":"num_iterations =100\n\n# Initialize a list to store the minimum number of non-zero features required\nmin_non_zero_features = []\n\n# Iterate over a range of iterations\nfor _ in range(num_iterations):\n    # Choose a random row index from X_test\n    random_row_index = np.random.randint(len(X_test))\n    random_row = X_test.iloc[random_row_index]\n\n    # Initialize the number of non-zero features\n    num_non_zero_features = np.count_nonzero(random_row)\n\n    # Initialize a flag to indicate if a significant drop in accuracy is observed\n    significant_drop = False\n\n    # Iterate over each feature and set it to zero one by one\n    for i, value in enumerate(random_row):\n        if value != 0:\n            # Incremental change: set the feature to zero\n            random_row_modified = random_row.copy()\n            random_row_modified[i] = 0\n\n            # Predict labels for the modified test data\n            y_pred_modified = knn_model.predict([random_row_modified])\n\n            # Calculate accuracy for the modified test data\n            accuracy_modified = accuracy_score([y_test.iloc[random_row_index]], y_pred_modified)\n\n            # Check if accuracy has dropped significantly\n            if accuracy_modified < initial_accuracy * 0.95:  # Adjust threshold as needed\n                min_non_zero_features.append(max(num_non_zero_features - i, 0))  # Ensure non-negative value\n                significant_drop = True\n                break\n\n    # Check if no significant drop in accuracy was observed\n    if not significant_drop:\n        min_non_zero_features.append(num_non_zero_features)\n\n# Find the minimum non-zero features required\nmin_non_zero_features_required = max(filter(None, min_non_zero_features), default=None)\n\n# Print the result\nif min_non_zero_features_required is not None:\n    print(\"Minimum number of non-zero features required:\", min_non_zero_features_required)\nelse:\n    print(\"No significant drop in accuracy observed for any random row.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I think the 2nd method is more reliable...In this method I choose a random row of xtest as a new customer and count the number of non zero features(number of products) and step by step remove a product by changing the column to zero; then calculate the accuracy and find the stage in which the accuracy drops; it means we find out that how many products is needed so that the model can predict the cluster correctly. by num_iterations, the procedure repeats and we choose the max value for a better trusted value. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}